from __future__ import annotations
import os
from pathlib import Path
from typing import List, Dict
from dataclasses import dataclass

from dotenv import load_dotenv
from docx import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.output_parsers import JsonOutputParser



@dataclass
class OOPPrinciple:
    """
    Data class representing an object-oriented principle detected in the project.

    Attributes:
        present (bool): Whether the principle is present in the project
        description (str): Description of the principle
        code_snippets (List[str]): List of code snippets, each with
        keys "file" and "code".
    """
    present: bool
    description: str
    code_snippets: List[Dict[str, str]]


@dataclass
class ResumeItem:
    """
    Data class representing a project resume item generated by the AI analysis.
    
    Attributes:
        project_title (str): The title/name of the project
        one_sentence_summary (str): Brief one-line description of the project
        detailed_summary (str): Comprehensive description of the project
        key_responsibilities (List[str]): List of main tasks/roles in the project
        key_skills_used (List[str]): List of skills demonstrated in the project
        tech_stack (str): Main technologies and tools used in the project
        impact (str): The significance or results achieved by the project
    """
    project_title: str
    one_sentence_summary: str
    detailed_summary: str
    key_responsibilities: List[str]
    key_skills_used: List[str]
    tech_stack: str
    impact: str
    oop_principles_detected: Dict[str, OOPPrinciple]


class GenerateProjectResume:
    IMAGE_EXTS = {".png", ".jpg", ".jpeg", ".gif", ".bmp", ".tiff", ".webp"}

    TEXT_EXTS = {".txt", ".md", ".rst", ".log"}

    CODE_EXTS = {
        # Python
        ".py", ".pyw", ".ipynb",".pyscript",

        # Web & JavaScript/TypeScript
        ".js", ".jsx", ".mjs", ".cjs", ".ts", ".tsx",

        # Java & JVM Languages
        ".java", ".kt", ".kts", ".scala", ".groovy",

        # C/C++ Family
        ".c", ".h", ".cpp", ".hpp", ".cc", ".cxx",

        # .NET
        ".cs", ".vb",

        # Web & Markup
        ".html", ".htm",
        ".css", ".scss", ".sass", ".less",
        ".vue", ".svelte",
        ".xml", ".svg",

        # Scripting
        ".rb", ".pl", ".pm", ".t", ".py",
        ".lua", ".r", ".R", ".m", ".jl",
        ".sh", ".bash", ".zsh", ".ps1", ".psm1", ".bat", ".cmd",
        ".php",

        # Systems & Compiled
        ".go", ".rs", ".swift", ".dart",
        ".zig", ".nim",
        ".m", ".mm",  # Objective-C/Objective-C++
        ".d", ".pas",
        ".f", ".f90", ".f95",  # Fortran
        ".asm", ".s",  # Assembly
        ".cu", ".cuh",  # CUDA
        ".cl",  # OpenCL

        # Configuration & Data
        ".json", ".yaml", ".yml", ".toml", ".ini", ".cfg",
        ".env", ".properties",
        ".sql", ".hql", ".cql",  # Database
        ".graphql", ".gql",
        ".tf", ".tfvars", ".hcl",  # Terraform

        # Build & Package Management
        ".gradle", ".pom.xml", ".csproj", ".fsproj", ".vbproj",
        ".cabal", ".hs-boot",
        ".cabal", ".hs-boot",
        ".nuspec",
        ".sln", ".vcxproj",
        ".bazel", ".bzl",
        ".buck",
        ".buckconfig",
        ".gn", ".gni"
    }



    def __init__(self,folder):

        """
        Initialize a GenerateProjectResume instance.

        :param folder: The root directory of the project
        :raises FileNotFoundError: If the given folder does not exist
        :raises RuntimeError: If the GOOGLE_API_KEY environment variable is not set
        """
        load_dotenv()
        self.google_model="gemini-2.5-flash" #Define what Gemini model we are going to be using
        self.folder=folder
        self.max_chars: int = 20_000
        self.project_root=Path(self.folder) #Getting the root folder path
        if not self.project_root.exists():
            raise FileNotFoundError(f"No such folder {self.project_root}")

        self.google_api_key = os.getenv("GOOGLE_API_KEY") #Get the Google_API_key stored in the .env file

        # If it's not added in the .env file, its raises a runtime error
        if not self.google_api_key:
            raise RuntimeError("Missing GOOGLE_API_KEY in .env file")


        #Here we are creating an instance of the LLM (langchain_google_genai.ChatGoogleGenerativeAI)
        # and passing in the model and the Google_api_key to authenticate the LLM.
        self.llm = ChatGoogleGenerativeAI(
            model=self.google_model,
            google_api_key=self.google_api_key
        )

        # Here we are creating an instance of the parser to parse the output from the LLM into JSON format
        self.parser = JsonOutputParser()



        """
        Here we are creating the prompt template to be sent to the LLM
        which in this case in gemini-2.5-flash
        """
        self.langChain_prompt = PromptTemplate.from_template(
            """
            You are an expert technical résumé writer.

            You are given a snapshot of a software project including:
            - file structure
            - code snippets (any language including PHP)
            - documentation (PDF/DOCX/TXT)
            - configuration files

            Return a single JSON object summarizing the entire project:

            {{
              "project_title": "...",
              "one_sentence_summary": "...",
              "detailed_summary": "...",
              "key_responsibilities": [
                "bullet point...",
                "bullet point..."
              ],
              "key_skills_used": [
                "skill...",
                "technology..."
              ],
              "tech_stack": "short paragraph of main technologies" and ,
              "impact": "optional short impact statement",
              "oop_principles_detected": {{
                "abstraction": {{
                  "present": false,
                  "description": "",
                  "code_snippets": []
                }},
                "encapsulation": {{
                  "present": false,
                  "description": "",
                  "code_snippets": []
                }},
                "inheritance": {{
                  "present": false,
                  "description": "",
                  "code_snippets": []
                }},
                "polymorphism": {{
                  "present": false,
                  "description": "",
                  "code_snippets": []
                }}
              }}
            }}

            For "key_skills_used", include both technical tools (languages, frameworks, libraries, databases, etc.)
            and conceptual skills (e.g., testing strategies, design patterns, cloud, CI/CD).
            When the project demonstrates object-oriented design, explicitly include object-oriented principles
            such as abstraction, encapsulation, inheritance, and polymorphism as skills where appropriate.

            For "oop_principles_detected":
            - For each principle (abstraction, encapsulation, inheritance, polymorphism):
              - Set "present" to true or false (JSON booleans).
              - If present, fill "description" with a short explanation referencing the file/class/method.
              - If present, "code_snippets" MUST be a non-empty array of objects like:
                {{
                  "file": "relative/path/to/file.ext",
                  "code": "EXACT code excerpt that demonstrates the principle"
                }}.
              - The "code" field MUST contain a direct copy of the relevant code from the project context
                (class definitions, methods, interface implementations, overridden methods, etc.).
              - Do NOT wrap code in markdown fences (no ```), only plain text inside the JSON string.
            - If a principle is not detected, leave:
              - "present": false
              - "description": ""
              - "code_snippets": []

            Do NOT output anything except valid JSON.

            PROJECT CONTEXT:
            \"\"\"{project_context}\"\"\"
            """
        )

        self.chain=self.langChain_prompt | self.llm | self.parser


    def _is_text_file(self, path: Path) -> bool:
        try:
            with path.open("rb") as f:
                f.read(2048).decode("utf-8")
            return True
        except Exception:
            return False


    def _classify_file(self, path: Path) -> str:
        """

        Classify a file by its extension or contents.

        Returns one of the following strings:
        - "ignore": if the file is an image or cannot be read
        - "pdf": if the file is a PDF
        - "docx": if the file is a DOCX or DOC
        - "text": if the file is a plaintext file
        - "code": if the file is a source code file or contains code snippets

        :param path: The path to the file to classify
        :type path: Path
        :return: A string indicating the type of the file
        :rtype: str
        """

        #Here we are getting the file extension type
        file_extension=path.suffix.lower()

        if file_extension in self.IMAGE_EXTS:
            return "ignore"

        if file_extension == ".pdf":
            return "pdf"

        if file_extension == ".docx" or file_extension == '.doc':
            return "docx"

        if file_extension in self.TEXT_EXTS:
            return "text"

        if file_extension in self.CODE_EXTS:
            return "code"

        if self._is_text_file(path):
            return "code"


        return "ignore"


    def _read_file(self,path:Path)->str:
        """
        Reads the contents of a file.

        The type of the file is determined by calling `_classify_file`.

        If the file is a PDF, it is read using PyPDFLoader.
        If the file is a DOCX, it is read using Python-Docx.
        If the file is a plaintext file or contains code snippets, it is read using Path.read_text.

        Returns the contents of the file as a string.
        If an exception occurs during file reading, an empty string is returned.

        :param path: The path to the file to read
        :type path: Path
        :return: The contents of the file as a string
        :rtype: str
        """
        # Here we are using the classify_file function to determine the type of the file
        ftype=self._classify_file(path)
        try:
            if ftype=="pdf":
                pdf_to_read = PyPDFLoader(str(path)).load()
                pdf_content = "\n".join(p.page_content for p in pdf_to_read)
                content=pdf_content
            elif ftype=="docx":
                docx_to_read = Document(str(path))
                doc_content="\n".join(d.text for d in docx_to_read.paragraphs)
                content=doc_content
            elif ftype in {"text", "code"}:
                content=path.read_text(errors="ignore")

            else:
                return ""

            return content
        except Exception:
            return ""

    def _build_context(self):
        """
        Builds a string context from the project root and its files.

        The context string is formatted as follows:
        - ROOT: <project root path>\n
        - FILES:\n
        - <file path> [<file type>]\n
        - SNIPPETS:\n
        - === <file path> [<file type>] ===\n
        - <file snippet>\n
        - === <file path> [<file type>] ===\n
        - ...

        If the total length of the context string exceeds `max_chars`, the remaining snippets are truncated.

        :return: The context string
        :rtype: str
        """
        pieces: List[str] = [f"ROOT: {self.project_root.resolve()}\n", "FILES:\n"]
        total_length=0
        file_infos=[]

        for f in sorted(self.project_root.glob("**/*")):
            if f.is_file() and self._classify_file(f)!='ignore':
                ftype=self._classify_file(f)
                save_dict={"path":f,"type":ftype}
                file_infos.append(save_dict)
                pieces.append(f"- {f.relative_to(self.project_root)} [{ftype}]\n")
        pieces.append("\nSNIPPETS:\n")


        for info in file_infos:
            path=info.get("path")
            ftype=info.get("type")
            rel=path.relative_to(self.project_root)

            raw=self._read_file(path)
            if not raw.strip():
                continue
            snippet=raw.strip()[:1200]
            block = f"\n=== {rel} [{ftype}] ===\n{snippet}\n"
            if total_length+len(block)>self.max_chars:
                pieces.append("\n[TRUNCATED]\n")
                break

            pieces.append(block)
            total_length+=len(block)

        return "".join(pieces)


    def generate(self)->ResumeItem:
        """
        Generates a resume item for the given project root.

        The resume item is generated by running the AI chain on the project context.

        The project context is built by recursively traversing the project root and its files,
        and extracting snippets from the files. The snippets are then fed into the AI chain,
        which generates a resume item based on the snippets.

        :return: The generated resume item
        :rtype: ResumeItem
        """
        print(f"running analysis on {self.project_root.name}")
        context=self._build_context()
        result=self.chain.invoke({"project_context":context}) #Here we are invoke the llm to start the analysis process
        raw_oop = result.get("oop_principles_detected", {}) or {}
        oop_principles: Dict[str, OOPPrinciple] = {}
        for name,data in raw_oop.items():
            if not isinstance(data, dict):
                continue
            oop_principles[name] = OOPPrinciple(
                present=bool(data.get("present", False)),
                description=data.get("description", "") or "",
                code_snippets=data.get("code_snippets", []) or [],

            )

        # Create ResumeItem from the LLM's JSON response with safe dictionary access
        resume_item = ResumeItem(
            # Project title from the analysis
            project_title=result.get("project_title", ""),
            # Brief project summary in one sentence
            one_sentence_summary=result.get("one_sentence_summary", ""),
            # Detailed project description
            detailed_summary=result.get("detailed_summary", ""),
            # List of main responsibilities/tasks
            key_responsibilities=result.get("key_responsibilities", []),
            # Technologies and skills demonstrated
            key_skills_used=result.get("key_skills_used", []),
            # Main technologies used in the project
            tech_stack=result.get("tech_stack", ""),
            # Project's impact or achievements
            impact=result.get("impact", ""),
            # Object-oriented principles detected in the project
            oop_principles_detected=oop_principles,

        )
        return resume_item



"""
docker = GenerateProjectResume(r"").generate()
print(docker.project_title)
print(docker.one_sentence_summary)
print(docker.key_skills_used)
print(docker.tech_stack)
print(docker.oop_principles_detected.keys())

for name, principle in docker.oop_principles_detected.items():
    print(f"=== {name.upper()} ===")
    print("present:", principle.present)
    print("description:", principle.description)
    for snippet in principle.code_snippets:
        print("file:", snippet.get("file"))
        print("code:", snippet.get("code")[:200], "...")


"""
